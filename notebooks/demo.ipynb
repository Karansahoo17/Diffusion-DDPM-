{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM (Denoising Diffusion Probabilistic Models) Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow of training and sampling from a DDPM model.\n",
    "\n",
    "## Overview\n",
    "- Load and preprocess data\n",
    "- Initialize DDPM model and U-Net\n",
    "- Train the model\n",
    "- Generate samples\n",
    "- Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from PIL import Image\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model.unet import UNet\n",
    "from model.diffusion import DDPM\n",
    "from data.dataset import get_dataset\n",
    "from training.trainer import DDPMTrainer\n",
    "from utils.scheduler import NoiseScheduler\n",
    "from utils.visualization import plot_samples, plot_loss_curve, plot_noise_schedule\n",
    "from utils.helpers import set_seed, save_checkpoint, load_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(config['training']['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = get_dataset(\n",
    "    dataset_name=config['data']['dataset'],\n",
    "    image_size=config['data']['image_size'],\n",
    "    data_dir=config['data']['data_dir'],\n",
    "    train=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['data']['num_workers']\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {config['data']['dataset']}\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Image size: {config['data']['image_size']}\")\n",
    "print(f\"Batch size: {config['training']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "sample_batch = next(iter(train_loader))\n",
    "images = sample_batch[0] if isinstance(sample_batch, tuple) else sample_batch\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    img = np.clip(img, 0, 1)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Training Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize U-Net model\n",
    "model = UNet(\n",
    "    in_channels=config['model']['in_channels'],\n",
    "    out_channels=config['model']['out_channels'],\n",
    "    base_channels=config['model']['base_channels'],\n",
    "    channel_mults=config['model']['channel_mults'],\n",
    "    num_res_blocks=config['model']['num_res_blocks'],\n",
    "    time_emb_dim=config['model']['time_emb_dim'],\n",
    "    dropout=config['model']['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Initialize noise scheduler\n",
    "noise_scheduler = NoiseScheduler(\n",
    "    num_timesteps=config['diffusion']['num_timesteps'],\n",
    "    beta_start=config['diffusion']['beta_start'],\n",
    "    beta_end=config['diffusion']['beta_end'],\n",
    "    schedule=config['diffusion']['schedule']\n",
    ")\n",
    "\n",
    "# Initialize DDPM\n",
    "ddpm = DDPM(\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Timesteps: {config['diffusion']['num_timesteps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Noise Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot noise schedule\n",
    "plot_noise_schedule(noise_scheduler)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstrate Forward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate forward diffusion process\n",
    "sample_image = images[0].unsqueeze(0).to(device)  # Take first image\n",
    "timesteps_to_show = [0, 50, 100, 200, 400, 800]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(15, 3))\n",
    "\n",
    "for i, t in enumerate(timesteps_to_show):\n",
    "    if t == 0:\n",
    "        noisy_image = sample_image\n",
    "    else:\n",
    "        t_tensor = torch.tensor([t], device=device)\n",
    "        noise = torch.randn_like(sample_image)\n",
    "        noisy_image = ddpm.q_sample(sample_image, t_tensor, noise)\n",
    "    \n",
    "    img = noisy_image[0].permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img + 1) / 2  # Denormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f't={t}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Forward Diffusion Process')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = DDPMTrainer(\n",
    "    ddpm=ddpm,\n",
    "    train_loader=train_loader,\n",
    "    config=config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "losses = trainer.train()\n",
    "\n",
    "# Plot training loss\n",
    "plot_loss_curve(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "print(\"Generating samples...\")\n",
    "ddpm.model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate a batch of samples\n",
    "    samples = ddpm.sample(\n",
    "        batch_size=16,\n",
    "        shape=(config['model']['in_channels'], \n",
    "               config['data']['image_size'], \n",
    "               config['data']['image_size'])\n",
    "    )\n",
    "\n",
    "# Visualize generated samples\n",
    "plot_samples(samples, nrow=4, title=\"Generated Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Sampling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the reverse sampling process\n",
    "print(\"Visualizing sampling process...\")\n",
    "\n",
    "ddpm.model.eval()\n",
    "with torch.no_grad():\n",
    "    # Start with pure noise\n",
    "    shape = (1, config['model']['in_channels'], \n",
    "             config['data']['image_size'], \n",
    "             config['data']['image_size'])\n",
    "    \n",
    "    x = torch.randn(shape, device=device)\n",
    "    \n",
    "    # Sample with intermediate steps\n",
    "    timesteps_to_show = [999, 800, 600, 400, 200, 100, 50, 0]\n",
    "    intermediate_samples = []\n",
    "    \n",
    "    for i in tqdm(reversed(range(ddpm.noise_scheduler.num_timesteps)), desc=\"Sampling\"):\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        x = ddpm.p_sample(x, t)\n",
    "        \n",
    "        if i in timesteps_to_show:\n",
    "            intermediate_samples.append(x.clone())\n",
    "    \n",
    "    # Plot intermediate samples\n",
    "    fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(16, 2))\n",
    "    \n",
    "    for i, (sample, t) in enumerate(zip(intermediate_samples, timesteps_to_show)):\n",
    "        img = sample[0].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f't={t}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Reverse Sampling Process')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Real vs Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real and generated images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "# Real images (top row)\n",
    "for i in range(8):\n",
    "    img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Real', rotation=0, ha='right', va='center')\n",
    "\n",
    "# Generated images (bottom row)\n",
    "for i in range(8):\n",
    "    img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Generated', rotation=0, ha='right', va='center')\n",
    "\n",
    "plt.suptitle('Real vs Generated Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint_path = '../checkpoints/ddpm_demo.pth'\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "save_checkpoint({\n",
    "    'model_state_dict': ddpm.model.state_dict(),\n",
    "    'config': config,\n",
    "    'losses': losses\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "# Save sample images\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "torchvision.utils.save_image(\n",
    "    samples, \n",
    "    '../results/generated_samples.png', \n",
    "    nrow=4, \n",
    "    normalize=True, \n",
    "    value_range=(-1, 1)\n",
    ")\n",
    "\n",
    "print(\"Generated samples saved to ../results/generated_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "print(\"\\nModel Analysis:\")\n",
    "print(f\"Final training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in ddpm.model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in ddpm.model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "# Training statistics\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"Number of epochs: {config['training']['epochs']}\")\n",
    "print(f\"Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"Total training steps: {len(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and preprocessing data\n",
    "2. Initializing DDPM model components\n",
    "3. Training the denoising model\n",
    "4. Generating new samples\n",
    "5. Visualizing the diffusion process\n",
    "6. Comparing real vs generated images\n",
    "\n",
    "The DDPM model learns to reverse the noise process by predicting the noise added at each timestep, enabling generation of high-quality samples from pure noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
