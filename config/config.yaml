# DDPM Configuration File

# Model Configuration
model:
  image_size: 32
  in_channels: 3
  model_channels: 128
  out_channels: 3
  num_res_blocks: 2
  attention_resolutions: [16, 8]
  dropout: 0.1
  channel_mult: [1, 2, 2, 2]
  num_heads: 4
  num_head_channels: 32
  use_scale_shift_norm: true
  resblock_updown: true

# Diffusion Configuration
diffusion:
  timesteps: 1000
  beta_schedule: "linear"  # "linear", "cosine", "sigmoid"
  beta_start: 0.0001
  beta_end: 0.02
  model_mean_type: "epsilon"  # "epsilon", "x0", "v"
  model_var_type: "fixed_small"  # "fixed_small", "fixed_large", "learned"
  loss_type: "mse"  # "mse", "l1", "huber"

# Training Configuration
training:
  batch_size: 128
  learning_rate: 2e-4
  num_epochs: 1000
  warmup_steps: 1000
  gradient_clip_val: 1.0
  ema_decay: 0.9999
  save_interval: 100
  log_interval: 10
  sample_interval: 50
  num_sample_images: 64

# Data Configuration
data:
  dataset: "cifar10"  # "cifar10", "celeba", "custom"
  data_path: "./data"
  image_size: 32
  num_workers: 4
  pin_memory: true
  
# Augmentation
augmentation:
  horizontal_flip: true
  random_crop: false
  normalize: true
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]

# Paths
paths:
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
  sample_dir: "./samples"
  results_dir: "./results"

# Device Configuration
device:
  use_cuda: true
  mixed_precision: true
  
# Optimizer Configuration
optimizer:
  type: "AdamW"
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler Configuration
scheduler:
  type: "cosine"  # "cosine", "linear", "step"
  eta_min: 1e-6
